# Default values for inference-gateway.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ghcr.io/inference-gateway/inference-gateway
  pullPolicy: IfNotPresent
  tag: ""

monitoring:
  enabled: false
  metricsPort: 9464
  namespaceSelector:
    matchNames:
      - monitoring

# Secrets and configurations references
envFrom:
  configMapRef:
  secretRef:

# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ""
fullnameOverride: ""

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This is for setting Kubernetes Annotations to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {}
# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 8080

# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: "selfsigned-issuer"
  hosts:
    - host: api.inference-gateway.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
    enabled: false
    # Hosts can be specified as either:
    # hosts: api.inference-gateway.local  # Single host
    # or
    hosts:
      - api.inference-gateway.local
    secretName: api-inference-gateway-tls

resources:
  limits:
    cpu: 200m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi

# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /health
    port: http
readinessProbe:
  httpGet:
    path: /health
    port: http

# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

# Extra environment variables to add to the deployment
extraEnv: []
# - name: EXAMPLE_VAR
#   value: "example-value"

config:
  # General settings
  ENVIRONMENT: "production"
  ENABLE_TELEMETRY: "false"
  ENABLE_AUTH: "false"
  # Model Context Protocol (MCP)
  MCP_ENABLE: "false"
  MCP_EXPOSE: "false"
  MCP_SERVERS: ""
  MCP_CLIENT_TIMEOUT: "5s"
  MCP_DIAL_TIMEOUT: "3s"
  MCP_TLS_HANDSHAKE_TIMEOUT: "3s"
  MCP_RESPONSE_HEADER_TIMEOUT: "3s"
  MCP_EXPECT_CONTINUE_TIMEOUT: "1s"
  MCP_REQUEST_TIMEOUT: "5s"
  # OpenID Connect
  OIDC_ISSUER_URL: "http://keycloak:8080/realms/inference-gateway-realm"
  # Server settings
  SERVER_HOST: "0.0.0.0"
  SERVER_PORT: "8080"
  SERVER_READ_TIMEOUT: "30s"
  SERVER_WRITE_TIMEOUT: "30s"
  SERVER_IDLE_TIMEOUT: "120s"
  SERVER_TLS_CERT_PATH: ""
  SERVER_TLS_KEY_PATH: ""
  # Client settings
  CLIENT_TIMEOUT: "30s"
  CLIENT_MAX_IDLE_CONNS: "20"
  CLIENT_MAX_IDLE_CONNS_PER_HOST: "20"
  CLIENT_IDLE_CONN_TIMEOUT: "30s"
  CLIENT_TLS_MIN_VERSION: "TLS12"
  # Providers
  ANTHROPIC_API_URL: "https://api.anthropic.com/v1"
  CLOUDFLARE_API_URL: "https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai"
  COHERE_API_URL: "https://api.cohere.ai"
  GROQ_API_URL: "https://api.groq.com/openai/v1"
  OLLAMA_API_URL: "http://ollama:8080/v1"
  OPENAI_API_URL: "https://api.openai.com/v1"
  DEEPSEEK_API_URL: "https://api.deepseek.com"
