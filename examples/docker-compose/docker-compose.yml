---
services:
  inference-gateway:
    build:
      context: ../../
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 100M
        reservations:
          memory: 100M
          cpus: "0.1"
    networks:
      - app_network
      - external_network

  # inference-gateway-debug:
  #   build:
  #     context: ../../
  #     dockerfile: Dockerfile.debug
  #   ports:
  #     - 8080:8080
  #     - 2345:2345
  #   security_opt:
  #     - "seccomp:unconfined"
  #   cap_add:
  #     - SYS_PTRACE
  #   env_file:
  #     - .env
  #   networks:
  #     - app_network

  curl:
    image: alpine/curl:latest
    networks:
      - app_network
      - external_network

  ollama:
    image: ollama/ollama:latest
    user: 65534:65534
    read_only: true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "ollama serve"
    environment:
      OLLAMA_HOST: 0.0.0.0:8080
      OLLAMA_DEBUG: 1
      HOME: /
    volumes:
      - type: volume
        source: docker-compose-ollama-data
        target: /.ollama
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8G
        reservations:
          memory: 4G
          cpus: "3"
    depends_on:
      ollama-model-downloader:
        condition: service_completed_successfully
    networks:
      - app_network

  ollama-model-downloader:
    image: ollama/ollama:latest
    entrypoint: ["/bin/sh", "-c"]
    environment:
      HOME: /
      OLLAMA_HOST: 127.0.0.0:8080
      OLLAMA_MAX_LOADED_MODELS: 0
    command: >
      "ollama serve & sleep 5 && ollama pull deepseek-r1:1.5b && chown -R 65534:65534 /.ollama"
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 512M
        reservations:
          memory: 512M
          cpus: "0.5"
    volumes:
      - type: volume
        source: docker-compose-ollama-data
        target: /.ollama

  # llama:
  #   image: ghcr.io/ggerganov/llama.cpp:server
  #   user: 65534:65534
  #   read_only: true
  #   cap_drop:
  #     - ALL
  #   cap_add:
  #     - NET_BIND_SERVICE
  #   ulimits:
  #     nofile:
  #       soft: 65536
  #       hard: 65536
  #   environment:
  #     LLAMA_ARG_MODEL: /models/deepseek-coder-6.7b-instruct.Q4_K_M.gguf
  #     LLAMA_ARG_CTX_SIZE: 4096
  #     LLAMA_ARG_N_PARALLEL: 2
  #     LLAMA_ARG_ENDPOINT_METRICS: 1
  #     LLAMA_ARG_HOST: 0.0.0.0
  #     LLAMA_ARG_PORT: 8080
  #   volumes:
  #     - type: volume
  #       source: docker-compose-llama-data
  #       target: /models
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "4.0"
  #         memory: 8G
  #       reservations:
  #         memory: 4G
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 40s
  #   networks:
  #     - llama_internal
  #   depends_on:
  #     model-downloader:
  #       condition: service_completed_successfully

  # model-downloader:
  #   image: python:3.14.0a4-alpine3.21
  #   cap_drop:
  #     - ALL
  #   cap_add:
  #     - NET_BIND_SERVICE
  #   volumes:
  #     - type: volume
  #       source: docker-compose-llama-data
  #       target: /models
  #   environment:
  #     - REPOSITORY_NAME=deepseek-ai
  #     - MODEL_NAME=DeepSeek-R1-Distill-Llama-8B
  #     - MODEL_FILE_NAME=DeepSeek-R1-Distill-Llama-8B
  #     - HF_TOKEN=${HF_TOKEN}
  #   command: >
  #     sh -c 'cd /models &&
  #       apk update &&
  #       pip install --upgrade pip &&
  #       pip3 install huggingface_hub &&
  #       huggingface-cli login --token $HF_TOKEN &&
  #       if [ ! -f $MODEL_FILE_NAME ]; then
  #         echo "Model $MODEL_NAME not found. Downloading model $MODEL_NAME"
  #         huggingface-cli download $REPOSITORY_NAME/$MODEL_NAME $MODEL_FILE_NAME --local-dir . --local-dir-use-symlinks False
  #       fi'

volumes:
  docker-compose-llama-data:
  docker-compose-ollama-data:

networks:
  app_network:
    internal: true
  external_network:
    driver: bridge
    internal: false
