---
services:
  inference-gateway:
    image: inference-gateway/inference-gateway:latest
    ports:
      - 8080:8080
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 100M
        reservations:
          memory: 100M
          cpus: "0.1"
    depends_on:
      ollama-health-checker:
        condition: service_healthy
    networks:
      - app_network
      - external_network
    pull_policy: always

  ollama-health-checker:
    image: alpine/curl:latest
    entrypoint:
      - /bin/sh
      - -c
      - "tail -f /dev/null"
    healthcheck:
      test:
        - CMD-SHELL
        - "curl -f http://ollama:8080/api/version || exit 1"
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - app_network

  ollama:
    image: ollama/ollama:latest
    entrypoint:
      - /bin/sh
      - -c
    command: >
      "
      ollama serve
      "
    working_dir: /root
    environment:
      OLLAMA_HOST: 0.0.0.0:8080
      OLLAMA_DEBUG: 0
      OLLAMA_KEEP_ALIVE: "10m"
      OLLAMA_MAX_LOADED_MODELS: 2
      OLLAMA_FLASH_ATTENTION: 1
      OLLAMA_NUM_PARALLEL: 6
      OLLAMA_LLM_LIBRARY: cpu
      OLLAMA_MAX_QUEUE: 512
      OLLAMA_RUNNERS_DIR: /tmp
      OLLAMA_TMPDIR: /tmp
      HOME: /root
      OLLAMA_REQUEST_TIMEOUT: 300s
      OLLAMA_CONCURRENT_REQUESTS: 4
    volumes:
      - type: volume
        source: docker-compose-ollama-data
        target: /root/.ollama
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 2G
    deploy:
      resources:
        limits:
          cpus: "6"
          memory: 8G
        reservations:
          memory: 6G
          cpus: "5"
    networks:
      - app_network

  ollama-model-downloader:
    image: ollama/ollama:latest
    entrypoint:
      - /bin/sh
      - -c
    command: >
      "
      ollama serve &
      sleep 10 &&
      ollama pull deepseek-r1:1.5b &&
      ollama pull qwen3:0.6b
      "
    volumes:
      - type: volume
        source: docker-compose-ollama-data
        target: /root/.ollama
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
      OLLAMA_MAX_LOADED_MODELS: 0
    networks:
      - app_network
      - external_network
    depends_on:
      ollama-health-checker:
        condition: service_healthy

volumes:
  docker-compose-ollama-data:

networks:
  app_network:
    internal: true
  external_network:
    internal: false
