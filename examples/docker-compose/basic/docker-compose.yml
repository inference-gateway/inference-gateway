---
services:
  inference-gateway:
    image: ghcr.io/inference-gateway/inference-gateway:latest
    env_file:
      - .env
    ports:
      - '8080:8080'
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 100M
    pull_policy: always
    restart: unless-stopped
