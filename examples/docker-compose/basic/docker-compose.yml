---
services:
  inference-gateway:
    build:
      context: ../../..
    env_file:
      - .env
    ports:
      - '8080:8080'
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 100M
    pull_policy: always
    restart: unless-stopped
