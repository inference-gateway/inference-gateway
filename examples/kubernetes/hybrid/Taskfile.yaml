---
version: "3"

tasks:
  deploy-infrastructure:
    desc: "Deploy hybrid infrastructure"
    cmds:
      - ctlptl apply -f Cluster.yaml
      - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace kube-system \
          --set controller.progressDeadlineSeconds=500 \
          --version 4.12.2 \
          --wait \
          ingress-nginx ingress-nginx/ingress-nginx
      - echo "ðŸš€ Hybrid infrastructure deployed successfully!"

  deploy-ollama:
    desc: "Deploy deepseek-r1:1.5b with Ollama"
    cmds:
      - kubectl create namespace ollama -o yaml --dry-run=client | kubectl apply -f -
      - kubectl apply -f ollama/
      - kubectl -n ollama rollout status deployment ollama
      - echo "ðŸ¤– Ollama with deepseek model deployed successfully!"

  watch-ollama-download:
    desc: "Watch download progress"
    cmds:
      - kubectl -n ollama logs deployments/ollama -c ollama-model-puller -f
      - echo "ðŸ‘€ Watching Ollama model download progress..."

  deploy-inference-gateway:
    desc: "Deploy inference-gateway with hybrid providers"
    cmds:
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace inference-gateway \
          --set ingress.enabled=true \
          --set config.OLLAMA_API_URL="http://ollama.ollama:8080/v1" \
          --set envFrom.secretRef=inference-gateway \
          --version 0.6.3 \
          --wait \
          inference-gateway oci://ghcr.io/inference-gateway/charts/inference-gateway
      - echo "ðŸšª Inference Gateway with hybrid providers deployed successfully!"

  clean:
    desc: "Clean up the cluster"
    cmds:
      - ctlptl delete -f Cluster.yaml
      - echo "ðŸ§¹ Cluster cleaned up successfully!"
