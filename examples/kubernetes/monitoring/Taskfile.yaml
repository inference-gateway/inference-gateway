---
version: "3"

tasks:
  deploy-infrastructure:
    desc: "Deploy monitoring infrastructure"
    cmds:
      - ctlptl apply -f Cluster.yaml
      - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
      - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      - helm repo add grafana https://grafana.github.io/helm-charts
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace kube-system \
          --version 4.12.1 \
          --wait \
          ingress-nginx ingress-nginx/ingress-nginx
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace monitoring \
          --version 70.4.2 \
          --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
          --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
          --set-string prometheus.prometheusSpec.serviceMonitorNamespaceSelector.matchLabels.monitoring=true \
          --set prometheus.enabled=false \
          --set alertmanager.enabled=false \
          --set kubeStateMetrics.enabled=false \
          --set nodeExporter.enabled=false \
          --set grafana.enabled=false \
          --wait \
          prometheus-operator prometheus-community/kube-prometheus-stack
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace monitoring \
          --version 5.17.1 \
          --wait \
          grafana-operator grafana/grafana-operator
      - kubectl label namespace monitoring monitoring="true" --overwrite
      - kubectl apply -f prometheus/
      - kubectl apply -f grafana/

  deploy-inference-gateway:
    desc: "Deploy inference-gateway with monitoring and autoscaling enabled"
    cmds:
      - |
        helm upgrade --install \
          --create-namespace \
          --namespace inference-gateway \
          --set config.ENABLE_TELEMETRY=true \
          --set config.OLLAMA_API_URL="http://ollama.ollama:8080/v1" \
          --set monitoring.enabled=true \
          --set ingress.enabled=true \
          --set autoscaling.enabled=true \
          --set autoscaling.minReplicas=2 \
          --set autoscaling.maxReplicas=10 \
          --set autoscaling.targetCPUUtilizationPercentage=80 \
          --set autoscaling.targetMemoryUtilizationPercentage=80 \
          --wait \
          inference-gateway oci://ghcr.io/inference-gateway/charts/inference-gateway:latest
      - kubectl label namespace inference-gateway monitoring="true" --overwrite

  deploy-ollama:
    desc: "Deploy ollama as a provider"
    cmds:
      - kubectl create namespace ollama --dry-run=client -o yaml | kubectl apply -f -
      - |
        kubectl apply -f ollama/
      - kubectl -n ollama rollout status deployment ollama

  simulate-requests:
    desc: "Generate test requests to simulate monitoring"
    cmds:
      - |
        for i in $(seq 1 20); do
          curl -s -X POST http://api.inference-gateway.local/v1/chat/completions \
            -H "Content-Type: application/json" \
            -d '{"model":"ollama/deepseek-r1:1.5b","messages":[{"role":"user","content":"Say hello"}]}' \
            --max-time 10
          echo
          sleep 1
        done

  clean:
    desc: "Clean up the cluster"
    cmds:
      - ctlptl delete -f Cluster.yaml
